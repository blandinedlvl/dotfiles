{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blandinedlvl/dotfiles/blob/master/D06_C01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwWzgZGOlmt3"
      },
      "source": [
        "# Fundamentals of Deep Learning -  Playground"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIaVmiDVlmt8"
      },
      "source": [
        "üß† Welcome to this module of Deep Learning!\n",
        "\n",
        "üéØ In this challenge, our goal is two-fold:\n",
        "1. Get a visual representation of Neural Networks\n",
        "2. Build a better intuition of what Neural Networks are doing\n",
        "\n",
        "üëâ We will use ***[Tensorflow Playground](https://playground.tensorflow.org/)***\n",
        "\n",
        "_(This first challenge does not require much code_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A2Vxy6vlmt-"
      },
      "source": [
        "## Classification in Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg0zrRPGlmt_"
      },
      "source": [
        "### (1) The data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTianaEBlmuA"
      },
      "source": [
        "‚ùì Let's to on the [Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=2&seed=0.23545&showTestData=false&discretize=false&percTrainData=70&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&showTestData_hide=false&stepButton_hide=false&activation_hide=false&problem_hide=false&batchSize_hide=true&dataset_hide=false&resetButton_hide=false&discretize_hide=false&playButton_hide=false&learningRate_hide=true&regularizationRate_hide=true&percTrainData_hide=false&numHiddenLayers_hide=false) and select the following type of data ‚ùì \n",
        "\n",
        "- A classification problem \n",
        "- The circle dataset (<span style=\"color:blue\">blue dots</span> inside a circle of <span style=\"color:orange\">orange dots</span>)\n",
        "- Ratio of training to test data : $ 70 \\% $\n",
        "- No noise ($ = 0$)\n",
        "- Do not show test data (right panel) \n",
        "- Do not discretize the output\n",
        "- Activation function: ***ReLU*** \n",
        "\n",
        "<details>\n",
        "    <summary><i> Why Relu ? </i></summary>\n",
        "        \n",
        "üí°In general, try it by default. It appears to work better for many problems!\n",
        "    \n",
        "_Note: Playground  allows you to select only **one** activation function used for **all** the **hidden** layers_\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0K-LZWhlmuC"
      },
      "source": [
        "### (2) The features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4QFamQplmuC"
      },
      "source": [
        "‚ùì <u>Questions about the features</u> ‚ùì\n",
        "\n",
        "1. Select only the features $X_1$ and $X_2$ (_unselect the other features if necessary_)\n",
        "2. If you were using the other variables such as $X_1^{2}$, $X_2^{2}$, $X_1 X_2$, $sin(X_1)$ and $sin(X_2)$, what type of classic Machine Learning operation does it corresponds to?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "challengify"
        ],
        "id": "15ar_dadlmuD"
      },
      "source": [
        "> FEATURE ENGINEERING "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejyUZ1hLlmuE"
      },
      "source": [
        "<details>\n",
        "    <summary><i>Answer</i></summary>\n",
        "\n",
        "* It corresponds to some type of ***feature engineering*** where you transform them. \n",
        "    * <i>Examples: multiplication, sinus, square, ...</i>\n",
        "* Here, in this exercise but also tomorrow, we will only use the raw input features $X_1$ and $X_2$. \n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFtiSMBGlmuF"
      },
      "source": [
        "### (3) Building and Fitting a Neural Network in ***Playground***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl0ZEuUzlmuG"
      },
      "source": [
        "‚ùì <u>Questions about Neural Networks</u> ‚ùì \n",
        "\n",
        "* üß† Build a model with the following architecture:\n",
        "    - three hidden layers\n",
        "    - 5 neurons on the first hidden layer\n",
        "    - 4 neurons on the second hidden layer\n",
        "    - 3 neurons on the last hidden layer\n",
        "    - In ***Playground***, the output layer is not represented: \n",
        "        - For such binary classification task, keep in mind that it will automatically be a dense layer with 1 neuron activated by the sigmoid function $ \\large \\phi(z) = \\frac{1}{1 + e^{-z}} $\n",
        "\n",
        "* üí™ ***Fit it and stop the iterations when the loss function has stabilized.***\n",
        "\n",
        "* üëÄ Observe carefully:\n",
        "    - Look at the individual neurons and try to understand what each neuron has specialized into during the _.fit()_\n",
        "    - What do you think about the overall shape your results? Re-run the neural network with different activation functions to compare. Can you make it work with \"Linear\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "challengify"
        ],
        "id": "Hlmts0fvlmuH"
      },
      "source": [
        "> YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSxcL_ublmuH"
      },
      "source": [
        "<details>\n",
        "    <summary><i>Answer: some insights about the activation functions</i></summary>\n",
        "\n",
        "- Results may look like an hexagon because ReLu is piece-wise linear!\n",
        "- A non-linearly separable problem cannot be fitted with a linear activation such as ***Linear***     \n",
        "- A non-linearly separable problem can surprisingly be well fitted with  a piece-wise linear activation function such as ***ReLu*** or ***LeakyReLu*** (even if that is not always true)\n",
        "- The ***tanh*** activation gives a \"smoother\" decision boundary.\n",
        "- The ***sigmoid*** does seem to work well here. \n",
        "    \n",
        "üßëüèª‚Äçüè´ Always start with ReLu, it's a safe bet üßëüèª‚Äçüè´!\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VLFWX4-lmuI"
      },
      "source": [
        "### (4) Building and Fitting a Neural Network in ***Tensorflow.Keras***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3jZvOkrlmuI"
      },
      "source": [
        "üëá We write for you the same model - at least the architecture - in Tensor Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh03Z-ktlmuI",
        "outputId": "e7068888-edd7-4987-e8d5-3e9b2560139e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-16 11:41:04.551518: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Dense(5, activation='relu', input_dim=2)) # 1st hidden layer with 5 neurons\n",
        "model.add(layers.Dense(4, activation='relu')) # 2nd hidden layer with 4 neurons\n",
        "model.add(layers.Dense(3, activation='relu')) # 3rd hidden layer with 3 neurons\n",
        "\n",
        "\n",
        "\n",
        "model.add(layers.Dense(1, activation='sigmoid')) # Output layer that outputs a probability of belonging\n",
        "                                                 # to the class of \"success\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ALIp4EelmuL"
      },
      "source": [
        "<details>\n",
        "    <summary><i>What to understand about the code of a Neural Network? </i>üëÜ</summary>\n",
        "\n",
        "- <u>First Hidden Layer a.k.a ***Input Layer***</u>:\n",
        "    - Every datapoint which will be analyzed by the neural network has two features $ X = \\begin{bmatrix} \n",
        "           X_{1} \\\\\n",
        "           X_{2} \\\\\n",
        "         \\end{bmatrix} $.\n",
        "    - You need to inform your Neural Network about the ***number of input features*** through the ***`input_dim` argument***\n",
        "    - A Neural Network tries to mimic the human brain. Here we would like to use 5 neurons to start analyzing each of these points.\n",
        "    \n",
        "    \n",
        "- <u>Second Hidden Layer</u>:\n",
        "    - Every datapoint went through the first hidden layer which was built using 5 neurons $ layer_1 = \\begin{bmatrix} \n",
        "           a_{1} \\\\\n",
        "           a_{2} \\\\\n",
        "           a_{3} \\\\\n",
        "           a_{4} \\\\\n",
        "           a_{5} \\\\           \n",
        "         \\end{bmatrix} $\n",
        "         \n",
        "    - What if we want to ***make the information flow*** through a second hidden layer with 4 neurons ? It is totally possible!\n",
        "    - These 4 neurons of the  $ layer_2 = \\begin{bmatrix} \n",
        "           b_{1} \\\\\n",
        "           b_{2} \\\\\n",
        "           b_{3} \\\\\n",
        "           b_{4} \\\\ \n",
        "         \\end{bmatrix} $of the second layer will analyze the 5 neurons from the first layer\n",
        "    \n",
        "- <u>Third Hidden Layer</u>:\n",
        "    - Every neuron of the second layer went through the third hidden layer which was built using 3 neurons $ layer_3 = \\begin{bmatrix} \n",
        "           c_{1} \\\\\n",
        "           c_{2} \\\\\n",
        "           c_{3} \n",
        "         \\end{bmatrix} $\n",
        "         \n",
        "    - What if we want to ***make the information flow*** through a second hidden layer with 4 neurons ? It is totally possible!\n",
        "    - These 3 neurons analyzed the neurons of the  $ layer_2  $ !\n",
        "\n",
        "- <u>Predictive Layer</u>\n",
        "    - You are dealing with a binary classification task\n",
        "    - We could use two neurons to predict the probability of belonging to class A or class B...\n",
        "    - But one neuron predicting the probability of \"succcess\" is enough\n",
        "\n",
        "- <u>About activation functions</u>\n",
        "    - Despite its simplicity, the ***ReLU*** has proven to be very effective to remove some linearity within the neurons\n",
        "    - For the predictive layer, the best activation function to use for a classification task is the ***sigmoid*** function. That is something we've already discussed during Decision Science and Machine Learning.\n",
        "\n",
        "- <u>About the Sequential aspect of the Network</u>:\n",
        "    - The fact that you are defining a **Sequential** model  here has this consequence:  the following layer is aware of its input size based on the output size of the previous layers !\n",
        "    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVLMqO18lmuM"
      },
      "source": [
        "‚ùì How many parameters are involved in this small Neural Network ‚ùì"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "challengify"
        ],
        "id": "jE7zVmL-lmuM",
        "outputId": "694b24b3-dd07-4499-c82b-be4073b28947"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 5)                 15        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 24        \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3)                 15        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 4         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 58\n",
            "Trainable params: 58\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH7ofLcolmuN"
      },
      "source": [
        "<details>\n",
        "    <summary><i>Hint</i></summary>\n",
        "\n",
        "‚úÖ You should have 58 parameters\n",
        "    \n",
        "‚ùå If not, double-check your architecture    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49CTO3iylmuN"
      },
      "source": [
        "### (5) The XOR Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdE_ze9DlmuN"
      },
      "source": [
        "‚ùì <u>Playing with the XOR Dataset</u> ‚ùì \n",
        "\n",
        "* On Playground:\n",
        "    - Change the dataset to the \"XOR - Exclusive Or\".\n",
        "    - Try to design a model with two hidden layers that has a very small **test loss** \n",
        "        - Note: you are free to choose the number of neurons per layer yourself.  \n",
        "        \n",
        "* Coding with Tensorflow/Keras:\n",
        "    - Once you have built your model on Playground, write it down  below with the Tensorflow/Keras library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRnAWLUTlmuN"
      },
      "outputs": [],
      "source": [
        "# Neural Network that can be well fitted to the XOR Dataset\n",
        "\n",
        "pass  # YOUR CODE HERE\n",
        "\n",
        "\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Dense(5, activation='relu', input_dim=2)) # 1st hidden layer with 5 neurons\n",
        "model.add(layers.Dense(4, activation='relu')) # 2nd hidden layer with 4 neurons\n",
        "\n",
        "model.add(layers.Dense(1, activation='relu')) # Output layer that outputs a probability of belonging\n",
        "                                                 # to the class of \"success\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9U00YYzlmuO",
        "outputId": "3be954f0-8137-437c-d106-dd25e951cf7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_25 (Dense)            (None, 5)                 15        \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 4)                 24        \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 44\n",
            "Trainable params: 44\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKfZlDWOlmuO"
      },
      "source": [
        "### (6) The Spiral Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u1d9IxRlmuO"
      },
      "source": [
        "‚ùì <u>Playing with the Spiral Dataset</u> ‚ùì \n",
        "\n",
        "* On Playground:\n",
        "    - Change the dataset to the \"Spiral\".\n",
        "    - Try to design a model with two hidden layers that has a very small **test loss** \n",
        "        - Note: you are free to choose the number of neurons per layer yourself.  \n",
        "        \n",
        "* Coding with Tensorflow/Keras:\n",
        "    - Once you have built your model on Playground, write it down  below with the Tensorflow/Keras library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4VhI8uPlmuP"
      },
      "outputs": [],
      "source": [
        "# Neural Network that can be well fitted to the Spiral Dataset\n",
        "\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Dense(5, activation='relu', input_dim=2)) # 1st hidden layer with 5 neurons\n",
        "model.add(layers.Dense(4, activation='relu')) # 2nd hidden layer with 4 neurons\n",
        "\n",
        "model.add(layers.Dense(1, activation='tanh')) # Output layer that outputs a probability of belonging\n",
        "                                                 # to the class of \"success\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWjLbZtYlmuP",
        "outputId": "bfbcd88c-05cc-4fe6-80ff-3aa297844921"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_28 (Dense)            (None, 5)                 15        \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 4)                 24        \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 44\n",
            "Trainable params: 44\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8x6du4-lmuP"
      },
      "source": [
        "### (7) How Deep should a Neural Network be ? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0VG6wEXlmuP"
      },
      "source": [
        "üëÄ If you compare the number of parameters needed to fit the Spiral Dataset vs. the XOR dataset, the former requires much more weights....\n",
        "\n",
        "üòÉ Actually, if your models are deep enough, you could potentially fit pretty much any pattern...\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "    <summary><i>Should I create Very Deep Neural Networks ? </i></summary>\n",
        "        \n",
        "<u>Examples:</u>\n",
        "    \n",
        "* Think about a human being. The more this person spends time coding in Python, the better he/she will get better at it !\n",
        "    \n",
        "* Think about a student. The more this person studies, the better he/she will pass exams. But sometimes students can study \"too much\" about a topic and forget about the global picture of a course....\n",
        "    \n",
        "<u>Lessons</u>\n",
        "    \n",
        "üß† For Deep Learning Models, the more layers they have, the more time they will have to learn patterns about the data...\n",
        "\n",
        "‚ùóÔ∏èThe problem is about avoiding **overfitting** ‚ùóÔ∏è\n",
        "    \n",
        "‚ò†Ô∏è Add a good deal of noise and you _may_ see that your model will have learned \"too much\" about this noise. \n",
        "  \n",
        "    \n",
        "üìÜ The next lecture **Deep Learning > Optimizers, Fit Loss** is dedicated to understand which techniques we could use to prevent Deep Learning models from overfitting.\n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "    <summary><i>A picture of overfitting in Playground</i></summary>\n",
        "    \n",
        "<img src='https://github.com/lewagon/data-images/blob/master/DL/playground-overfitting.png?raw=true' width=700 style='margin:auto'>\n",
        "</details>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVqIhjaQlmuQ"
      },
      "source": [
        "## Regression in Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhswjWbslmuQ"
      },
      "source": [
        "<u>Let's try to complete a Regression Task using Deep Learning</u>\n",
        "\n",
        "\n",
        "This time, the last layer will no longer look like:  \n",
        "```python\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "```\n",
        "\n",
        "but instead  :\n",
        "```python\n",
        "model.add(layers.Dense(1, activation='linear'))\n",
        "```\n",
        "\n",
        "meaning that the output of this network is no longer between $0$ and $1$ (probability) but between $ -\\infty$ and $+ \\infty$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D6RkpuwlmuQ"
      },
      "source": [
        "‚ùì <u>Playing with the Regression Dataset</u> ‚ùì \n",
        "\n",
        "* On Playground:\n",
        "    - Change the dataset to the \"Regression\".\n",
        "    - Try to design a model that has a very small **test loss** \n",
        "        - Note: you are free to choose both the number of layers and the number of neurons per layer yourself \n",
        "        \n",
        "* Coding with Tensorflow/Keras:\n",
        "    - Once you have built your model on Playground, write it down  below with the Tensorflow/Keras library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NC-S_zklmuQ"
      },
      "outputs": [],
      "source": [
        "# Neural Network that can be well fitted to the Regression Dataset\n",
        "\n",
        "\n",
        "# Neural Network that can be well fitted to the Spiral Dataset\n",
        "\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Dense(5, activation='relu', input_dim=2)) # 1st hidden layer with 5 neurons\n",
        "model.add(layers.Dense(4, activation='relu')) # 2nd hidden layer with 4 neurons\n",
        "\n",
        "model.add(layers.Dense(1, activation='linear')) # Output layer that outputs a probability of belonging\n",
        "                                                 # to the class of \"success\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICfv76KVlmuR",
        "outputId": "046ea726-4067-4a01-ff98-67699a6a9fe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_31 (Dense)            (None, 5)                 15        \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 4)                 24        \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 44\n",
            "Trainable params: 44\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhXwlioRlmuR"
      },
      "source": [
        "üèÅ You are now ready to do the same things with TensorKeras directly !\n",
        "\n",
        "üí™ This was a Warm-Up about Neural Networks / Deep Learning Models... (even if, admittedly, our networks in this challenge were not so \"deep\"). \n",
        "\n",
        "\n",
        "üíæ Don't forget to `git add/commit/push` your notebook...\n",
        "\n",
        "üöÄ ... and move on to the next challenge !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dpL-rSQlmuR",
        "outputId": "8fec1d24-13d7-4d52-e4be-d2bf2e7ecfc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[master ba0f7d16] completed\n",
            " 1 file changed, 177 insertions(+), 14 deletions(-)\n",
            "√ânum√©ration des objets: 11, fait.\n",
            "D√©compte des objets: 100% (11/11), fait.\n",
            "Compression par delta en utilisant jusqu'√† 4 fils d'ex√©cution\n",
            "Compression des objets: 100% (6/6), fait.\n",
            "√âcriture des objets: 100% (6/6), 1.57 Kio | 537.00 Kio/s, fait.\n",
            "Total 6 (delta 4), r√©utilis√©s 0 (delta 0), r√©utilis√©s du pack 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To github.com:blandinedlvl/data-challenges.git\n",
            "   ceabee2f..ba0f7d16  master -> master\n"
          ]
        }
      ],
      "source": [
        "! git add playground.ipynb\n",
        "! git commit -m 'completed'\n",
        "! git push origin master "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0IihoEmlmuS"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "D06-C01",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}